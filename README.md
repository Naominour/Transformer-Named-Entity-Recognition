# Transformer Network Application: Named-Entity Recognition

This project focuses on applying **transformer models** to the task of **Named-Entity Recognition (NER)**. NER is a crucial component of **natural language processing (NLP)** that involves identifying and classifying key information (named entities) in text into predefined categories such as names of persons, organizations, locations, dates, and more.

**In this project, you will:**

Utilize tokenizers and pre-trained models from the HuggingFace Library.
Fine-tune a pre-trained transformer model specifically for the NER task.
Evaluate the performance of the fine-tuned model on an NER dataset.
The goal is to leverage the power of transformer-based models, which have shown state-of-the-art performance in various NLP tasks, to improve the accuracy and efficiency of NER systems. This involves loading a pre-trained model, preparing the dataset, fine-tuning the model, and evaluating its performance using relevant metrics.

<div align="center">
<img src="\NER.png" style="width:450px;"> <br>Designed by Freepik
</div>

![Deep Learning](https://img.shields.io/badge/Skill-Deep%20Learning-yellow)
![Natural Language Processing](https://img.shields.io/badge/Skill-Natural%20Language%20Processing-green)
![Transformer](https://img.shields.io/badge/Skill-Transformer-brightblue)
![TensorFlow](https://img.shields.io/badge/Skill-TensorFlow-orange)
![Keras](https://img.shields.io/badge/Skill-Keras-yellow)
![Python Programming](https://img.shields.io/badge/Skill-Python%20Programming-orange)

## Key Features
- Utilizes state-of-the-art transformer models for NER.
- Fine-tuning of pre-trained models for improved accuracy.
- Comprehensive evaluation of model performance.

## Frameworks and Libraries
![TensorFlow](https://img.shields.io/badge/TensorFlow-2.16.1-orange.svg?style=flat&logo=tensorflow)
![Keras](https://img.shields.io/badge/Keras-3.3.3-red.svg?style=flat&logo=keras)
![NumPy](https://img.shields.io/badge/NumPy-1.26.4-blue.svg?style=flat&logo=numpy)
![Matplotlib](https://img.shields.io/badge/Matplotlib-3.6.2-green.svg?style=flat&logo=matplotlib)

## Implementation Explanation
The project involves fine-tuning a pre-trained transformer model using the HuggingFace library. The process includes:

Tokenizing the input text.
Padding and encoding the sequences.
Training the model on the NER dataset.
Evaluating the model's performance using F1 score and classification report.

## Results

The fine-tuned transformer model achieves high accuracy in recognizing named entities in the provided text, demonstrating the effectiveness of using transformer-based models for NER tasks.